# ğŸ§  Decoder-Only Transformer (from Scratch)
*A lightweight educational PyTorch implementation with temperature, top-k, and top-p (nucleus) sampling.*

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BhrgvPtl/decoder-transformer/blob/main/decoder_transformer_sampling.ipynb)

---

## ğŸ“˜ Overview
This repository demonstrates how to **build and train a minimal decoder-only Transformer** (similar to GPT) from scratch using PyTorch.

Itâ€™s designed to be:
- ğŸ’¡ **Educational** â€” every line is easy to follow and fully commented.  
- âš¡ **Practical** â€” you can train and sample on any machine (CPU or GPU).  
- ğŸ”¬ **Extensible** â€” you can easily add datasets, larger models, or new sampling techniques.

---

## ğŸš€ Features

- Full decoder-only Transformer architecture implemented from scratch  
- Multi-head self-attention, feed-forward, and residual connections  
- Sinusoidal positional encodings  
- Token-level language modeling  
- Flexible sampling with **temperature**, **top-k**, and **top-p (nucleus)** options  
- Example notebook and CLI scripts for training and generation  

---

## ğŸ—‚ï¸ Repository Structure
```
decoder-transformer/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ decoder_transformer/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model.py           # Transformer model, attention, and training logic
â”‚       â””â”€â”€ sampling.py        # Top-k and top-p sampling utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py               # Train the Transformer on a toy dataset
â”‚   â””â”€â”€ generate.py            # Generate text from a trained checkpoint
â”œâ”€â”€ decoder_transformer_sampling.ipynb   # Colab notebook for quick experimentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ .env                       # Sets PYTHONPATH=./src (for VS Code & local runs)
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/BhrgvPtl/decoder-transformer.git
cd decoder-transformer
```

### 2ï¸âƒ£ Create and Activate Virtual Environment
```bash
python -m venv .venv
# Windows
.\.venv\Scripts ctivate
# macOS/Linux
source .venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

> âœ… Tip: VS Code will automatically load the `.env` file so imports work out of the box.

---

## ğŸ§© Training the Model

```bash
python scripts/train.py
```

Sample output:
```
Device: cpu
iter   50 | train loss 0.0373
iter  100 | train loss 0.0330
iter  150 | train loss 0.0315
iter  200 | train loss 0.0309
iter  250 | train loss 0.0310
iter  300 | train loss 0.0312
Saved: checkpoints/model.pt
```

---

## ğŸ’¬ Text Generation

Once training completes:
```bash
python scripts/generate.py --checkpoint checkpoints/model.pt --max_new_tokens 20 --temperature 0.9 --top_p 0.95
```

Example output:
```
Tokens: [2, 3, 4, 5, 0, 2, 3, 4, 5, 0, 2]
Decoded: <SOS> I Love Transformers <EOS> <SOS> I Love Transformers <EOS> <SOS>
```

---

## ğŸ›ï¸ Sampling Options

| Argument | Description | Example |
|-----------|-------------|----------|
| `--temperature` | Controls randomness (higher â†’ more random) | `--temperature 1.2` |
| `--top_k` | Keeps only top-k probable tokens | `--top_k 5` |
| `--top_p` | Nucleus sampling, keeps smallest cumulative prob â‰¥ p | `--top_p 0.9` |

You can combine them freely:
```bash
python scripts/generate.py --temperature 1.1 --top_k 10 --top_p 0.8
```

---

## ğŸ§  How It Works

1. **Tokenization & Embedding** â€“ Converts tokens to vectors and adds sinusoidal positional encodings.  
2. **Transformer Blocks** â€“ Each block has:
   - Causal self-attention  
   - Feed-forward MLP  
   - Residual connections + LayerNorm  
3. **Autoregressive Prediction** â€“ The model predicts the next token given all previous ones.  
4. **Sampling** â€“ Uses temperature scaling + top-k/top-p filtering to generate natural-looking text.

---

## ğŸ§ª Example: Training Curve & Output
| Iteration | Loss | Notes |
|------------|------|-------|
| 50 | 0.0037 | Model learning token dependencies |
| 100 | 0.0006 | Converging |
| 300 | 0.0001 | Stable, generates consistent phrases |

Generated sample:
```
<SOS> I Love Transformers <EOS> <SOS> Transformers are awesome <EOS>
```

---

## ğŸ§° Development & Contribution

### Run in VS Code
- Clone the repo and open it in VS Code  
- Ensure the Python extension is enabled  
- `.env` automatically sets `PYTHONPATH=./src`  

### Pre-commit Setup (optional)
```bash
pip install pre-commit
pre-commit install
```

### Contribute
1. Fork the repo  
2. Create a feature branch  
3. Commit and push your changes  
4. Submit a pull request ğŸ‰  

Ideas:
- Add dataset loading (WikiText, TinyStories)
- Add unit tests for `sampling.py`
- Add evaluation metrics (perplexity)
- Visualize attention weights

---

## ğŸ§¾ License
Licensed under the **MIT License** â€” see [LICENSE](./LICENSE).

---

## ğŸ™Œ Acknowledgements
- Inspired by [Andrej Karpathyâ€™s â€œminGPTâ€](https://github.com/karpathy/minGPT)  
- Built purely in PyTorch for transparency and learning  
- Special thanks to open-source contributors who make deep learning accessible  

---

### â­ If you find this useful, star the repo!
Your support helps grow the open-source ecosystem.
