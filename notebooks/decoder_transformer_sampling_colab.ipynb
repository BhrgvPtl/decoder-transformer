{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4L+aaaTFqwSiaRUH+/4Af",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhrgvPtl/decoder-transformer/blob/main/decoder_transformer_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #@title 🔧 Setup\n",
        "# You usually don't need to install torch on Colab; it's preinstalled.\n",
        "# If you want the absolute latest: uncomment the next line.\n",
        "# !pip install --quiet --upgrade torch\n",
        "\n",
        "import re\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = 24          # context window (max tokens per forward pass)\n",
        "    n_embd: int = 64              # embedding size (d_model)\n",
        "    n_heads: int = 4              # attention heads\n",
        "    n_layers: int = 2             # transformer blocks\n",
        "    dropout: float = 0.1\n",
        "    lr: float = 3e-4\n",
        "    batch_size: int = 16\n",
        "    max_iters: int = 300\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 1337\n",
        "\n",
        "    # Sampling controls\n",
        "    temperature: float = 1.0      # >0; lower = more deterministic\n",
        "    top_k: Optional[int] = None   # e.g., 5; None to disable\n",
        "    top_p: Optional[float] = None # e.g., 0.9; None to disable\n",
        "\n",
        "CFG = Config()\n",
        "torch.manual_seed(CFG.seed)\n",
        "print(\"Device:\", CFG.device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GENQqmj4Zsn",
        "outputId": "55ca11ed-8c3e-42c6-e25e-80c67e05863c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📚 Corpus + Dataloader\n",
        "\n",
        "# Small corpus (a few short lines so it trains fast)\n",
        "CORPUS = [\n",
        "    \"<SOS> I Love Transformers <EOS>\",\n",
        "    \"<SOS> Transformers are awesome <EOS>\",\n",
        "    \"<SOS> I Love attention mechanisms <EOS>\",\n",
        "    \"<SOS> Self attention learns token relations <EOS>\",\n",
        "]\n",
        "\n",
        "# Tokenize: keep <SOS>/<EOS>, words, punctuation\n",
        "def tokenize(s: str) -> List[str]:\n",
        "    return re.findall(r\"<EOS>|<SOS>|[\\w]+|[^\\w\\s]\", s)\n",
        "\n",
        "tokens_all: List[str] = []\n",
        "for line in CORPUS:\n",
        "    tokens_all.extend(tokenize(line))\n",
        "\n",
        "SPECIALS = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
        "for sp in SPECIALS:\n",
        "    if sp not in tokens_all:\n",
        "        tokens_all.append(sp)\n",
        "\n",
        "# Vocab\n",
        "itos = sorted(set(tokens_all))\n",
        "stoi = {t: i for i, t in enumerate(itos)}\n",
        "PAD_ID = stoi[\"<PAD>\"]\n",
        "SOS_ID = stoi[\"<SOS>\"]\n",
        "EOS_ID = stoi[\"<EOS>\"]\n",
        "VOCAB_SIZE = len(itos)\n",
        "\n",
        "# Encode the whole corpus multiple times to create a stream\n",
        "encoded_stream = []\n",
        "for _ in range(32):\n",
        "    for line in CORPUS:\n",
        "        encoded_stream.extend(stoi[t] for t in tokenize(line))\n",
        "encoded_data = torch.tensor(encoded_stream, dtype=torch.long)\n",
        "\n",
        "class LMWindowedDataset(Dataset):\n",
        "    \"\"\"Overlapping (x, y) windows of length block_size for next-token prediction.\"\"\"\n",
        "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
        "        self.ids = ids\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return max(0, len(self.ids) - self.block_size)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        chunk = self.ids[idx : idx + self.block_size + 1]\n",
        "        x = chunk[:-1]\n",
        "        y = chunk[1:]\n",
        "        return x, y\n",
        "\n",
        "train_ds = LMWindowedDataset(encoded_data, CFG.block_size)\n",
        "train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "print(\"Vocab size:\", VOCAB_SIZE)\n",
        "print(\"Dataset length:\", len(train_ds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-LpezbKAYdN",
        "outputId": "689382c9-a1a5-469e-e17a-034ebabe363b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 14\n",
            "Dataset length: 712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧠 Transformer Decoder Components\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 10_000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # (T, C)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (T, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
        "                             * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "        self.register_buffer(\"pe\", pe)  # not trainable\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_heads == 0, \"n_embd must be divisible by n_heads\"\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = n_embd // n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.k_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.v_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.out_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.resid_drop = nn.Dropout(dropout)\n",
        "\n",
        "        # Max mask size = CFG.block_size (we’ll slice to current T each forward)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(CFG.block_size, CFG.block_size)).unsqueeze(0).unsqueeze(0)\n",
        "        )  # shape: (1, 1, T, T)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.size()\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))  # (B, nh, T, T)\n",
        "        mask = self.mask[:, :, :T, :T]\n",
        "        att = att.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v  # (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.out_proj(y))\n",
        "        return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ff = FeedForward(n_embd, dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.ln1(x))  # residual 1\n",
        "        x = x + self.ff(self.ln2(x))    # residual 2\n",
        "        return x\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, n_embd: int, n_heads: int, n_layers: int,\n",
        "                 dropout: float, block_size: int):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(n_embd, max_len=block_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(n_embd, n_heads, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
        "        B, T = idx.shape\n",
        "        if T > self.block_size:\n",
        "            raise ValueError(f\"Sequence length {T} > block_size {self.block_size}\")\n",
        "\n",
        "        x = self.tok_emb(idx)     # (B, T, C)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, V)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1),\n",
        "                ignore_index=PAD_ID\n",
        "            )\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        idx: torch.Tensor,\n",
        "        max_new_tokens: int,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: Optional[int] = None,\n",
        "        top_p: Optional[float] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            next_token_logits = logits[:, -1, :]  # (B, V)\n",
        "\n",
        "            if temperature <= 0:\n",
        "                raise ValueError(\"temperature must be > 0\")\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            next_token_logits = top_k_top_p_filtering(\n",
        "                next_token_logits, top_k=top_k, top_p=top_p\n",
        "            )\n",
        "\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "1YutIUH0AigR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🎲 Sampling Filters (top-k / top-p)\n",
        "\n",
        "def top_k_top_p_filtering(\n",
        "    logits: torch.Tensor,\n",
        "    top_k: Optional[int] = None,\n",
        "    top_p: Optional[float] = None,\n",
        "    filter_value: float = -float(\"Inf\"),\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Apply top-k and/or nucleus (top-p) filtering per row to logits.\n",
        "\n",
        "    Args:\n",
        "        logits: (B, V)\n",
        "        top_k: keep only k largest logits\n",
        "        top_p: keep smallest set of tokens whose cumulative prob >= top_p\n",
        "    \"\"\"\n",
        "    B, V = logits.shape\n",
        "\n",
        "    # Top-k\n",
        "    if top_k is not None and 1 <= top_k < V:\n",
        "        kth_vals = torch.topk(logits, top_k, dim=-1).values[:, -1].unsqueeze(-1)  # (B, 1)\n",
        "        mask = logits < kth_vals\n",
        "        logits = logits.masked_fill(mask, filter_value)\n",
        "\n",
        "    # Top-p (nucleus)\n",
        "    if top_p is not None and 0.0 < top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)  # (B, V)\n",
        "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # mask tokens with cumulative prob > top_p (keep at least one)\n",
        "        nucleus_mask = cumprobs > top_p\n",
        "        nucleus_mask[:, 1:] = nucleus_mask[:, :-1].clone()\n",
        "        nucleus_mask[:, 0] = False\n",
        "\n",
        "        # map mask back to original indices\n",
        "        scatter_mask = torch.zeros_like(nucleus_mask, dtype=torch.bool).scatter_(1, sorted_indices, nucleus_mask)\n",
        "        logits = logits.masked_fill(scatter_mask, filter_value)\n",
        "\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "bLhhMzuoAoLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🚂 Train Helpers\n",
        "\n",
        "def train_one_epoch(model, loader, optim, device) -> float:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        _, loss = model(x, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def decode(ids: List[int]) -> str:\n",
        "    return \" \".join(itos[i] for i in ids)\n"
      ],
      "metadata": {
        "id": "QnRrNDtvArBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🏁 Train & Generate\n",
        "\n",
        "model = DecoderOnlyTransformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    n_embd=CFG.n_embd,\n",
        "    n_heads=CFG.n_heads,\n",
        "    n_layers=CFG.n_layers,\n",
        "    dropout=CFG.dropout,\n",
        "    block_size=CFG.block_size,\n",
        ").to(CFG.device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "\n",
        "for it in range(1, CFG.max_iters + 1):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, CFG.device)\n",
        "    if it % 50 == 0 or it == CFG.max_iters:\n",
        "        print(f\"iter {it:4d} | train loss {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylF-XMn1Ass6",
        "outputId": "94012d07-2b8b-434c-d4a5-c21efba86a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter   50 | train loss 0.0388\n",
            "iter  100 | train loss 0.0336\n",
            "iter  150 | train loss 0.0319\n",
            "iter  200 | train loss 0.0309\n",
            "iter  250 | train loss 0.0313\n",
            "iter  300 | train loss 0.0313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generation demos ---\n",
        "start = torch.tensor([[stoi[\"<SOS>\"]]], dtype=torch.long, device=CFG.device)\n",
        "\n",
        "print(\"\\n=== Greedy-ish (temperature=1.0, no top-k/p) ===\")\n",
        "out = model.generate(start.clone(), max_new_tokens=12, temperature=1.0, top_k=None, top_p=None)[0].tolist()\n",
        "print(out)\n",
        "print(\"Decoded:\", decode(out))\n",
        "\n",
        "print(\"\\n=== Temperature=0.8, top_k=5 ===\")\n",
        "out = model.generate(start.clone(), max_new_tokens=12, temperature=0.8, top_k=5, top_p=None)[0].tolist()\n",
        "print(out)\n",
        "print(\"Decoded:\", decode(out))\n",
        "\n",
        "print(\"\\n=== Temperature=1.2, top_p=0.9 (nucleus) ===\")\n",
        "out = model.generate(start.clone(), max_new_tokens=12, temperature=1.2, top_k=None, top_p=0.9)[0].tolist()\n",
        "print(out)\n",
        "print(\"Decoded:\", decode(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwPo_3lPAu-J",
        "outputId": "885841bb-19ac-400e-dae3-19790b258f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Greedy-ish (temperature=1.0, no top-k/p) ===\n",
            "[2, 6, 7, 9, 0, 2, 3, 4, 8, 11, 0, 2, 5]\n",
            "Decoded: <SOS> Transformers are awesome <EOS> <SOS> I Love attention mechanisms <EOS> <SOS> Self\n",
            "\n",
            "=== Temperature=0.8, top_k=5 ===\n",
            "[2, 3, 4, 6, 0, 2, 6, 7, 9, 0, 2, 3, 4]\n",
            "Decoded: <SOS> I Love Transformers <EOS> <SOS> Transformers are awesome <EOS> <SOS> I Love\n",
            "\n",
            "=== Temperature=1.2, top_p=0.9 (nucleus) ===\n",
            "[2, 3, 4, 8, 11, 0, 2, 5, 8, 10, 13, 12, 0]\n",
            "Decoded: <SOS> I Love attention mechanisms <EOS> <SOS> Self attention learns token relations <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3o9DwWQZA4M5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}